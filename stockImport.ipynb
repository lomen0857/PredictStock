{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os.path\n",
    "import jsm\n",
    "\n",
    "from pandas.core import common as com\n",
    "\n",
    "def set_span(start=None, end=None, periods=None, freq='D'):\n",
    "    \"\"\" 引数のstart, end, periodsに対して\n",
    "    startとendの時間を返す。\n",
    "\n",
    "    * start, end, periods合わせて2つの引数が指定されていなければエラー\n",
    "    * start, endが指定されていたらそのまま返す\n",
    "    * start, periodsが指定されていたら、endを計算する\n",
    "    * end, periodsが指定されていたら、startを計算する\n",
    "    \"\"\"\n",
    "    if com._count_not_none(start, end, periods) != 2:  # Like a pd.date_range Error\n",
    "        raise ValueError('Must specify two of start, end, or periods')\n",
    "    start = start if start else (pd.Period(end, freq) - periods).start_time\n",
    "    end = end if end else (pd.Period(start, freq) + periods).start_time\n",
    "    return start, end\n",
    "\n",
    "\n",
    "def get_jstock(code, freq='D', start=None, end=None, periods=None):\n",
    "    \"\"\"get Japanese stock data using jsm\n",
    "    Usage:\n",
    "        `get_jstock(6502)`\n",
    "        To get TOSHIBA daily from today back to 30days except holiday.\n",
    "\n",
    "        `get_jstock(6502, 'W', start=pd.Timestamp('2016'), end=pd.Timestamp('2017'))`\n",
    "        To get TOSHIBA weekly from 2016-01-01 to 2017-01-01.\n",
    "\n",
    "        `get_jstock(6502, end=pd.Timestamp('20170201'), periods=50)`\n",
    "        To get TOSHIBA daily from 2017-02-01 back to 50days except holiday.\n",
    "\n",
    "        `get_jstock(6502, 'M', start='first', end='last')`\n",
    "        To get TOSHIBA monthly from 2000-01-01 (the date of start recording) to today.\n",
    "    \"\"\"\n",
    "    # Default args\n",
    "    if com._count_not_none(start, end, periods) == 0:  # All of args is None\n",
    "        end, periods = 'last', 30\n",
    "\n",
    "    # Switch frequency Dayly, Weekly or Monthly\n",
    "    freq_dict = {'D': jsm.DAILY, 'W': jsm.WEEKLY, 'M': jsm.MONTHLY}\n",
    "\n",
    "    # 'first' means the start of recording date\n",
    "    if start == 'first':\n",
    "        data = jsm.Quotes().get_historical_prices(\n",
    "            code, range_type=freq_dict[freq], all=True)\n",
    "        start = [i.date for i in data][-1]\n",
    "    else:\n",
    "        data = None  # Temporaly defined\n",
    "\n",
    "    # 'last' means last weekday (or today)\n",
    "    if end == 'last':\n",
    "        end = pd.datetime.today()\n",
    "\n",
    "    # Return \"start\" and \"end\"\n",
    "    start, end = (x.date() if hasattr(x, 'date')\n",
    "                  else x for x in set_span(start, end, periods, freq))\n",
    "    print('Get data from {} to {}'.format(start, end))\n",
    "\n",
    "    data = jsm.Quotes().get_historical_prices(\n",
    "        code, range_type=freq_dict[freq], start_date=start, end_date=end) if not data else data\n",
    "    df = _convert_dataframe(data)\n",
    "    return df[start:end]\n",
    "\n",
    "\n",
    "def _convert_dataframe(target):\n",
    "    \"\"\"Convert <jsm.pricebase.PriceData> to <pandas.DataFrame>\"\"\"\n",
    "    date = [_.date for _ in target]\n",
    "    open = [_.open for _ in target]\n",
    "    high = [_.high for _ in target]\n",
    "    low = [_.low for _ in target]\n",
    "    close = [_.close for _ in target]\n",
    "    adj_close = [_._adj_close for _ in target]\n",
    "    volume = [_.volume for _ in target]\n",
    "    data = {'Open': open,\n",
    "            'High': high,\n",
    "            'Low': low,\n",
    "            'Close': close,\n",
    "            'Adj Close': adj_close,\n",
    "            'Volume': volume}\n",
    "    columns = *data.keys(),\n",
    "    df = pd.DataFrame(data, index=date, columns=columns).sort_index()\n",
    "    df.index.name = 'Date'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start:1305\n",
      "Get data from 2001-01-01 to 2019-12-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tamura\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\tamura\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "#学習用\n",
    "df = pd.read_csv('importETF.csv',encoding='utf8')\n",
    "errorStock_array = []\n",
    "\n",
    "for code in df['code']:\n",
    "    print(\"start:\" + str(code))\n",
    "    \n",
    "    #csvファイルの存在チェック\n",
    "    #path = 'StockData/' + str(code) + '過去データ.csv'\n",
    "    \n",
    "    try:\n",
    "        df_temp = get_jstock(code,start=pd.Timestamp('20010101'),end=pd.Timestamp('20191231'))\n",
    "        df_temp.to_csv('importETF/' + str(code) + '.csv')\n",
    "        print(\"end:\" + str(code))\n",
    "    except:\n",
    "        errorStock_array.append(code)\n",
    "        print(\"error\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1613',\n",
       " '1615',\n",
       " '1308',\n",
       " '1310',\n",
       " '1319',\n",
       " '1330',\n",
       " '1612',\n",
       " '1311',\n",
       " '1305',\n",
       " '1320',\n",
       " '1610',\n",
       " '1306',\n",
       " '1321']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xcolumns1 = pickle.load(open(\"max_Xcolumns1.sav\",\"rb\"))\n",
    "Xcolumns2 = pickle.load(open(\"max_Xcolumns2.sav\",\"rb\"))\n",
    "Xcolumns3 = pickle.load(open(\"max_Xcolumns3.sav\",\"rb\"))\n",
    "\n",
    "columns_array = ['1321']\n",
    "\n",
    "for index in range(len(Xcolumns1)):\n",
    "    columns_array.append(Xcolumns1[index][:4])\n",
    "\n",
    "for index in range(len(Xcolumns2)):\n",
    "    columns_array.append(Xcolumns2[index][:4])\n",
    "\n",
    "for index in range(len(Xcolumns3)):\n",
    "    columns_array.append(Xcolumns3[index][:4])\n",
    "\n",
    "    \n",
    "columns_unique_array = list(set(columns_array))\n",
    "columns_unique_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start:1613\n",
      "Get data from 2019-04-11 to 2019-04-12\n",
      "error\n",
      "start:1615\n",
      "Get data from 2019-04-11 to 2019-04-12\n",
      "end:1615\n",
      "start:1308\n",
      "Get data from 2019-04-11 to 2019-04-12\n",
      "end:1308\n",
      "start:1310\n",
      "Get data from 2019-04-11 to 2019-04-12\n",
      "end:1310\n",
      "start:1319\n",
      "Get data from 2019-04-11 to 2019-04-12\n",
      "end:1319\n",
      "start:1330\n",
      "Get data from 2019-04-11 to 2019-04-12\n",
      "end:1330\n",
      "start:1612\n",
      "Get data from 2019-04-11 to 2019-04-12\n",
      "end:1612\n",
      "start:1311\n",
      "Get data from 2019-04-11 to 2019-04-12\n",
      "end:1311\n",
      "start:1305\n",
      "Get data from 2019-04-11 to 2019-04-12\n",
      "end:1305\n",
      "start:1320\n",
      "Get data from 2019-04-11 to 2019-04-12\n",
      "end:1320\n",
      "start:1610\n",
      "Get data from 2019-04-11 to 2019-04-12\n",
      "error\n",
      "start:1306\n",
      "Get data from 2019-04-11 to 2019-04-12\n",
      "end:1306\n",
      "start:1321\n",
      "Get data from 2019-04-11 to 2019-04-12\n",
      "end:1321\n"
     ]
    }
   ],
   "source": [
    "#予想用\n",
    "\n",
    "from datetime import datetime, date, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os.path\n",
    "from sklearn import preprocessing\n",
    "\n",
    "Xcolumns1 = pickle.load(open(\"max_Xcolumns1.sav\",\"rb\"))\n",
    "Xcolumns2 = pickle.load(open(\"max_Xcolumns2.sav\",\"rb\"))\n",
    "Xcolumns3 = pickle.load(open(\"max_Xcolumns3.sav\",\"rb\"))\n",
    "\n",
    "columns_array = ['1321']\n",
    "\n",
    "for index in range(len(Xcolumns1)):\n",
    "    columns_array.append(Xcolumns1[index][:4])\n",
    "\n",
    "for index in range(len(Xcolumns2)):\n",
    "    columns_array.append(Xcolumns2[index][:4])\n",
    "\n",
    "for index in range(len(Xcolumns3)):\n",
    "    columns_array.append(Xcolumns3[index][:4])\n",
    "\n",
    "    \n",
    "columns_unique_array = list(set(columns_array))\n",
    "\n",
    "errorStock_array = []\n",
    "today = datetime.today()\n",
    "one = today - timedelta(days=2)\n",
    "one_str = datetime.strftime(one,'%Y-%m-%d')\n",
    "two = today - timedelta(days=3)\n",
    "two_str = datetime.strftime(two,'%Y-%m-%d')\n",
    "\n",
    "for code in columns_unique_array :\n",
    "    print(\"start:\" + str(code))\n",
    "    \n",
    "    try:\n",
    "        df_temp = get_jstock(code,start=pd.Timestamp(two_str),end=pd.Timestamp(one_str))\n",
    "        df_temp.to_csv('PredictImportETF/' + str(code) + '.csv')\n",
    "        print(\"end:\" + str(code))\n",
    "    except:\n",
    "        df_temp = pd.DataFrame(np.zeros([2,6]), columns=['Open','High','Low','Close','Adj Close','Volume'],index=[two_str,one_str])\n",
    "        df_temp.index.name = 'Date'\n",
    "\n",
    "        df_temp.to_csv('PredictImportETF/' + str(code) + '.csv')\n",
    "        errorStock_array.append(code)\n",
    "        print(\"error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1613\n",
      "1615\n",
      "1308\n",
      "1310\n",
      "1319\n",
      "1330\n",
      "1612\n",
      "1311\n",
      "1305\n",
      "1320\n",
      "1610\n",
      "1306\n",
      "1321\n"
     ]
    }
   ],
   "source": [
    "for code in columns_unique_array:\n",
    "    print(code)\n",
    "    \n",
    "    path = \"PredictImportETF/\" + str(code) + \".csv\"\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        temp_df = pd.read_csv(path,engine = \"python\" ,encoding=\"utf8\")\n",
    "    else:\n",
    "        errorStock_array.append(code)\n",
    "        continue\n",
    "        \n",
    "    new_df = pd.DataFrame()\n",
    "    new_df[\"Date\"] = temp_df[\"Date\"]\n",
    "    \n",
    "    #始値\n",
    "    new_df[\"Open\"] = 0\n",
    "    \n",
    "    for dateIndex in temp_df.index:\n",
    "        \n",
    "        #当日の始値\n",
    "        openValue = temp_df.at[dateIndex,\"Open\"]\n",
    "        \n",
    "        new_df.at[dateIndex,\"High\"] = temp_df.at[dateIndex,\"High\"] - openValue\n",
    "        new_df.at[dateIndex,\"Low\"] = temp_df.at[dateIndex,\"Low\"] - openValue\n",
    "        new_df.at[dateIndex,\"Close\"] = temp_df.at[dateIndex,\"Close\"] - openValue\n",
    "        \n",
    "        if dateIndex != 0:\n",
    "            new_df.at[dateIndex,\"Volume\"] = temp_df.at[dateIndex,\"Volume\"] - temp_df.at[dateIndex-1,\"Volume\"]\n",
    "            new_df.at[dateIndex,\"Open\"] = openValue - temp_df.at[dateIndex-1,\"Close\"]\n",
    "            \n",
    "        else:\n",
    "            new_df.at[0,\"Volume\"] = 0\n",
    "    \n",
    "    #csvファイル書き出し\n",
    "    new_df.to_csv(\"PredictStockDataDif/\" + str(code) + \"_dif.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ETF_df = pd.DataFrame()\n",
    "\n",
    "for code in columns_unique_array:\n",
    "    \n",
    "    code = str(code)\n",
    "    \n",
    "    temp = pd.DataFrame()\n",
    "    temp = pd.read_csv(\"PredictStockDataDif/\" + code + \"_dif.csv\",encoding=\"utf8\")\n",
    "    \n",
    "    if code == columns_unique_array[0]:\n",
    "        #初回のみETF_dfにindexを設定\n",
    "        ETF_df[\"Date\"] = temp[\"Date\"]\n",
    "        ETF_df = ETF_df.set_index(\"Date\")\n",
    "        \n",
    "    if code ==\"1321\":\n",
    "        for dateIndex in range(0,len(temp.index)-1):\n",
    "            tempDate = temp.at[dateIndex,\"Date\"]\n",
    "                \n",
    "            tempClose = temp.at[dateIndex+1,\"Close\"]\n",
    "            if tempClose >= 0:\n",
    "                ETF_df.at[tempDate,\"nextDay_HighLow\"] = 1\n",
    "            else:\n",
    "                ETF_df.at[tempDate,\"nextDay_HighLow\"] = -1\n",
    "                \n",
    "    for dateIndex in temp.index:\n",
    "        tempDate = temp.at[dateIndex,\"Date\"]\n",
    "            \n",
    "        ETF_df.at[tempDate,code + \"Open\"] = temp.at[dateIndex,\"Open\"]\n",
    "        ETF_df.at[tempDate,code + \"High\"] = temp.at[dateIndex,\"High\"]\n",
    "        ETF_df.at[tempDate,code + \"Low\"] = temp.at[dateIndex,\"Low\"]\n",
    "        ETF_df.at[tempDate,code + \"Close\"] = temp.at[dateIndex,\"Close\"]\n",
    "        ETF_df.at[tempDate,code + \"Volume\"] = temp.at[dateIndex,\"Volume\"]/10000\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1613', '1610']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errorStock_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['1613Open', '1613High', '1613Low', '1613Close', '1613Volume',\n",
       "       '1615Open', '1615High', '1615Low', '1615Close', '1615Volume',\n",
       "       '1308Open', '1308High', '1308Low', '1308Close', '1308Volume',\n",
       "       '1310Open', '1310High', '1310Low', '1310Close', '1310Volume',\n",
       "       '1319Open', '1319High', '1319Low', '1319Close', '1319Volume',\n",
       "       '1330Open', '1330High', '1330Low', '1330Close', '1330Volume',\n",
       "       '1612Open', '1612High', '1612Low', '1612Close', '1612Volume',\n",
       "       '1311Open', '1311High', '1311Low', '1311Close', '1311Volume',\n",
       "       '1305Open', '1305High', '1305Low', '1305Close', '1305Volume',\n",
       "       '1320Open', '1320High', '1320Low', '1320Close', '1320Volume',\n",
       "       '1610Open', '1610High', '1610Low', '1610Close', '1610Volume',\n",
       "       '1306Open', '1306High', '1306Low', '1306Close', '1306Volume',\n",
       "       'nextDay_HighLow', '1321Open', '1321High', '1321Low', '1321Close',\n",
       "       '1321Volume'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ETF_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ETF_df1 = ETF_df[Xcolumns1]\n",
    "ETF_df2 = ETF_df[Xcolumns2]\n",
    "ETF_df3 = ETF_df[Xcolumns3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ETF_df1 = ETF_df1.fillna(0)\n",
    "ETF_df2 = ETF_df2.fillna(0)\n",
    "ETF_df3 = ETF_df3.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n",
      "-1.0\n",
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "clf1 = pickle.load(open(\"max_clf1.sav\",\"rb\"))\n",
    "pred1 = clf1.predict(ETF_df1)\n",
    "print(pred1[1])\n",
    "\n",
    "clf2 = pickle.load(open(\"max_clf2.sav\",\"rb\"))\n",
    "pred2 = clf2.predict(ETF_df2)\n",
    "print(pred2[1])\n",
    "\n",
    "clf3 = pickle.load(open(\"max_clf3.sav\",\"rb\"))\n",
    "pred3 = clf3.predict(ETF_df3)\n",
    "print(pred3[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
